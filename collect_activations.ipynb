{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00558f82-c68d-4bb1-95dd-2153847fc32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1035295/1533163030.py:8: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"bool\"):\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Compatibility for older code referencing np.bool\n",
    "if not hasattr(np, \"bool\"):\n",
    "    np.bool = bool \n",
    "\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from nnsight import LanguageModel\n",
    "from crosscoder_learning.dictionary_learning.cache import ActivationCache\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdc73ed3-f4a4-41e9-9eb1-e31a223f9ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ActivationCollectionArgs:\n",
    "    # Model + dataset\n",
    "    model: str = \"/pscratch/sd/r/ritesh11/temp/Llama-3.2-3B-Instruct/\"\n",
    "    dataset: str = \"hf_dataset\"\n",
    "    trust: str = \"low_trust_dataset\"\n",
    "    dataset_split: str = \"train\"\n",
    "    text_column: str = \"text\"\n",
    "\n",
    "    # Activation collection\n",
    "    activation_store_dir: str = \"model_activations/llama\"\n",
    "    layers: List[int] = field(default_factory=lambda: [22])\n",
    "    batch_size: int = 1  # There appears to be a bug with batch size > 1\n",
    "    context_len: int = 3008\n",
    "    overwrite: bool = True\n",
    "    store_tokens: bool = True\n",
    "    disable_multiprocessing: bool = False\n",
    "\n",
    "    # Limits\n",
    "    num_samples: int = 10 ** 6\n",
    "    max_tokens: int = 10 ** 8\n",
    "\n",
    "    # Data type\n",
    "    dtype: str = \"bfloat16\"  # choices: \"bfloat16\", \"float16\", \"float32\"\n",
    "    random_seed: int = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f3cb0d7-2b25-49d8-8e2e-72ad2a84811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ActivationCollectionArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11210f6d-d475-42e5-bc0c-d21950e3697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtype_from_string(dtype_str: str) -> torch.dtype:\n",
    "    if dtype_str == \"bfloat16\":\n",
    "        return torch.bfloat16\n",
    "    if dtype_str == \"float16\":\n",
    "        return torch.float16\n",
    "    if dtype_str == \"float32\":\n",
    "        return torch.float32\n",
    "    raise ValueError(f\"Invalid dtype: {dtype_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0c6d563-bdad-494d-90f2-7d72d4cd8cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dtype = dtype_from_string(args.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c06fb101-1440-4bba-aa13-d66984cd0fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e967bea85bbc4bd0b8c1a2bd66362d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Environment setup\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Convert dtype string to torch dtype\n",
    "torch_dtype = dtype_from_string(args.dtype)\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    args.model,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch_dtype,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f709b6d-4ad7-4ea8-87a3-12540729ef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.layers = [i for i in range(11,21,2)]\n",
    "THINK_MODE = False\n",
    "\n",
    "if 'qwen' in args.model.lower():\n",
    "    THINK_MODE = True\n",
    "    args.layers = [i for i in range(13,27,2)]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0927f632-f133-49a0-99d5-07b66c35c6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 13, 15, 17, 19]\n",
      "False\n",
      "model_activations/llama\n"
     ]
    }
   ],
   "source": [
    "print(args.layers)\n",
    "print(THINK_MODE)\n",
    "print(args.activation_store_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf19fc32-f02b-4532-bd66-7e91506e9ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap with nnsight\n",
    "nnmodel = LanguageModel(model, tokenizer=tokenizer)\n",
    "\n",
    "# Submodules to trace\n",
    "submodules = [nnmodel.model.layers[layer_idx] for layer_idx in args.layers]\n",
    "submodule_names = [f\"layer_{layer_idx}\" for layer_idx in args.layers]\n",
    "d_model = nnmodel._model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cb490c3-f861-4bff-85cb-90cab841af4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_dir = Path(args.activation_store_dir)\n",
    "store_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "out_dir = store_dir / args.trust / args.dataset_split\n",
    "out_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57e4b998-6b63-4d2c-843d-0f4de05cc074",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(Path(args.dataset) / args.trust)\n",
    "dataset = dataset[args.dataset_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95e8d80f-e966-49e9-9a1a-0a0ccbd4fe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_user_turn(example):\n",
    "    return any(msg[\"role\"] == \"user\" for msg in example[\"messages\"])\n",
    "\n",
    "dataset = dataset.filter(has_user_turn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9842129-fc1c-41fa-a50e-df8b1938e3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_at_last_user(messages):\n",
    "    last_user_idx = max(\n",
    "        i for i, m in enumerate(messages) if m[\"role\"] == \"user\"\n",
    "    )\n",
    "    return messages[: last_user_idx + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "553f5acf-fbcb-4cb7-8764-c029fb52008e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSISTANT_START = \"<|im_start|>assistant\\n\"\n",
    "GEN_PREFIX = \"<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "\n",
    "def format_messages(example):\n",
    "    messages = example[\"messages\"]\n",
    "\n",
    "    # Truncate at last user\n",
    "    last_user_idx = max(\n",
    "        i for i, m in enumerate(messages) if m[\"role\"] == \"user\"\n",
    "    )\n",
    "    messages = messages[: last_user_idx + 1]\n",
    "\n",
    "    if not THINK_MODE:\n",
    "        # Vanilla behavior\n",
    "        formatted = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "        return {\"text\": formatted}\n",
    "\n",
    "    # THINK_MODE = True\n",
    "    formatted = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False,\n",
    "    )\n",
    "\n",
    "    # Inject <think> into historical assistant turns\n",
    "    formatted = formatted.replace(\n",
    "        ASSISTANT_START,\n",
    "        GEN_PREFIX\n",
    "    )\n",
    "\n",
    "    # Add clean generation prompt\n",
    "    formatted += ASSISTANT_START\n",
    "\n",
    "    return {\"text\": formatted}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2699ca7f-6c67-487d-bd32-642666afd848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f910134f3d394560bbbd3a41f59336db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/956 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(\n",
    "    format_messages,\n",
    "    remove_columns=dataset.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce234713-1517-4287-94b7-da2e4ecad27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not tokenizer.pad_token_id:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33ed80ba-ca88-4ea1-8f3a-8ddedf460847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting activations...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d597f1e7ef84e5a9e34b9e909e23696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting activations:   0%|          | 0/956 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing shard 0...\n",
      "Applying async save for shard 0 (current num of workers: 1)\n",
      "Applying async save for shard 0 (current num of workers: 2)\n",
      "Storing activation shard (torch.Size([705926, 3072]))\n",
      "Applying async save for shard 0 (current num of workers: 3)\n",
      "Storing activation shard (torch.Size([705926, 3072]))\n",
      "Storing activation shard (torch.Size([705926, 3072]))\n",
      "Finished storing activations for shard 0\n",
      "Applying async save for shard 0 (current num of workers: 3)\n",
      "Storing activation shard (torch.Size([705926, 3072]))\n",
      "Finished storing activations for shard 0\n",
      "Applying async save for shard 0 (current num of workers: 3)\n",
      "Storing tokens...\n",
      "Storing sequence ranges...\n",
      "Stored 957 sequence ranges\n",
      "Waiting for 3 save processes to finish\n",
      "Finished storing activations for shard 0\n",
      "Storing activation shard (torch.Size([705926, 3072]))\n",
      "Waiting for 2 save processes to finish\n",
      "Waiting for 2 save processes to finish\n",
      "Finished storing activations for shard 0\n",
      "Waiting for 1 save processes to finish\n",
      "Finished storing activations for shard 0\n",
      "Finished collecting activations. Total size: 705926\n"
     ]
    }
   ],
   "source": [
    "ActivationCache.collect(\n",
    "    dataset[args.text_column],\n",
    "    submodules,\n",
    "    submodule_names,\n",
    "    nnmodel,\n",
    "    out_dir,\n",
    "    shuffle_shards=False,\n",
    "    io=\"out\",\n",
    "    shard_size=10 ** 6,\n",
    "    batch_size=args.batch_size,\n",
    "    context_len=args.context_len,\n",
    "    d_model=d_model,\n",
    "    last_submodule=submodules[-1],\n",
    "    max_total_tokens=args.max_tokens,\n",
    "    store_tokens=args.store_tokens,\n",
    "    multiprocessing=not args.disable_multiprocessing,\n",
    "    ignore_first_n_tokens_per_sample=0,\n",
    "    overwrite=args.overwrite,\n",
    "    token_level_replacement=None,\n",
    "    add_special_tokens=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fbd457-aa6b-4b13-9500-9ed525efd598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

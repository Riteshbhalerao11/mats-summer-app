{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35c7cb6a-5bc0-49ad-ac00-19febb8a503f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n",
    "from nnsight import LanguageModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "from typing import List, Union, Tuple, Literal, Any, cast\n",
    "\n",
    "import yaml\n",
    "\n",
    "from steer_utils import (create_user_token_mask, create_system_token_mask, apply_steering_to_layer, \n",
    "                    get_model_info, prepare_steering_vectors, load_steering_vectors_from_npy, load_prompts_with_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dbe1ee-eea2-48a8-98dc-5524f2a98526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_steering_params(\n",
    "    layer_range: tuple[int, int],\n",
    "    num_layers: int,\n",
    "    effective_strength: float\n",
    ") -> tuple[list[int], float]:\n",
    "    \"\"\"\n",
    "    Calculate which layers to steer and the per-layer multiplier.\n",
    "    \"\"\"\n",
    "    # --- 1. Handle edge cases ---\n",
    "    assert num_layers <= layer_range[1] - layer_range[0]\n",
    "\n",
    "    if num_layers <= 0:\n",
    "        return [], 0.0\n",
    "\n",
    "    # --- 2. Calculate the per-layer multiplier ---\n",
    "    multiplier = effective_strength / num_layers\n",
    "\n",
    "    # --- 3. Select evenly spaced layers ---\n",
    "    start_layer, end_layer = layer_range\n",
    "\n",
    "    # If only one layer is requested, pick the middle of the range\n",
    "    if num_layers == 1:\n",
    "        middle_layer = round((start_layer + end_layer) / 2)\n",
    "        return [middle_layer], multiplier\n",
    "\n",
    "    # For multiple layers, calculate the step size to space them out evenly\n",
    "    # We divide by (num_layers - 1) to ensure the first and last points\n",
    "    # land on the start and end of the range.\n",
    "    step = (end_layer - start_layer) / (num_layers - 1)\n",
    "\n",
    "    layers_to_steer = [\n",
    "        round(start_layer + i * step) for i in range(num_layers)\n",
    "    ]\n",
    "\n",
    "    return layers_to_steer, multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e1b830-7cbc-45d2-86ea-24b5224334b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def steer_and_generate(\n",
    "    prompt_list: List[str],\n",
    "    system_prompt: Union[str, List[str]],\n",
    "    lma,\n",
    "    tokenizer,\n",
    "    steering_vectors: dict[torch.Tensor, float] | None,\n",
    "    batch_size: int,\n",
    "    max_new_tokens: int,\n",
    "    temperature: float,\n",
    "    layer_to_steer: int | Literal['all'] | List[int],\n",
    "    d_model: int,\n",
    "    steer_on_user: bool,\n",
    "    steer_on_thinking: bool,\n",
    "    steer_on_system: bool,\n",
    "    top_p: float,\n",
    "    resdir: str,\n",
    "    backup_every: int = 1,\n",
    ") -> Tuple[List[str], List[str], List[Any], List[Any]]:\n",
    "    \"\"\"\n",
    "    Generate steered responses for a list of prompts with optional user-token-only steering.\n",
    "    \n",
    "    Args:\n",
    "        prompt_list: List of prompts to process\n",
    "        system_prompt: System prompt(s) to use - can be a single string or list of strings\n",
    "        lma: LanguageModel instance\n",
    "        tokenizer: AutoTokenizer instance\n",
    "        steering_vectors: Dict mapping tensors to their multipliers (default: None)\n",
    "        batch_size: Number of prompts to process in each batch\n",
    "        max_new_tokens: Maximum number of new tokens to generate\n",
    "        temperature: Temperature for generation\n",
    "        layer_to_steer: Layer(s) to apply steering to\n",
    "        d_model: Model dimension\n",
    "        steer_on_user: Whether to steer on user prompt tokens\n",
    "        steer_on_thinking: Whether to steer on thinking tokens\n",
    "        steer_on_system: Whether to steer on system prompt tokens\n",
    "        top_p: Nucleus sampling parameter\n",
    "        resdir: Results directory (already includes model_name/priority/config path)\n",
    "                e.g., 'steered_outs/qwen_qwen3-32b/high-awareness/layer_10_strength_1p50/'\n",
    "        source_files: List of source filenames for each prompt\n",
    "        checksums: List of checksums for each prompt\n",
    "        buckets: List of bucket names for each prompt\n",
    "        seed: Random seed for generation\n",
    "        backup_every: How often to save results\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (full_responses, model_only_responses, tok_batches, out_steered_list)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(resdir, exist_ok=True)\n",
    "    \n",
    "    # Check which files already exist and verify checksums\n",
    "    indices_to_process = []\n",
    "    for idx, (source_file, checksum) in enumerate(zip(source_files, checksums)):\n",
    "        # Create output filename based on source file with seed\n",
    "        # Use basename to match save logic\n",
    "        base_name = os.path.splitext(os.path.basename(source_file))[0]\n",
    "        out_filename = f\"{base_name}_seed_{seed}_steer_out.yaml\"\n",
    "        out_path = os.path.join(resdir, out_filename)\n",
    "        \n",
    "        should_process = True\n",
    "        \n",
    "        if os.path.exists(out_path):\n",
    "            # File exists, check if checksum matches\n",
    "            try:\n",
    "                with open(out_path, 'r', encoding='utf-8') as f:\n",
    "                    existing_data = yaml.safe_load(f)\n",
    "                \n",
    "                existing_checksum = existing_data.get('checksum', '')\n",
    "                \n",
    "                if existing_checksum == checksum:\n",
    "                    # Checksum matches, skip this file\n",
    "                    should_process = False\n",
    "                else:\n",
    "                    # Checksum mismatch, reprocess\n",
    "                    print(f\"Checksum mismatch for {source_file}, will reprocess\")\n",
    "                    should_process = True\n",
    "            except Exception as e:\n",
    "                # Error reading file, reprocess to be safe\n",
    "                print(f\"Error reading {out_path}: {e}, will reprocess\")\n",
    "                should_process = True\n",
    "        \n",
    "        if should_process:\n",
    "            indices_to_process.append(idx)\n",
    "    \n",
    "    if len(indices_to_process) == 0:\n",
    "        print(\"All files already processed with matching checksums. Skipping...\")\n",
    "        return [], [], [], []\n",
    "    \n",
    "    print(f\"Processing {len(indices_to_process)} / {len(prompt_list)} prompts (skipping {len(prompt_list) - len(indices_to_process)} existing)\")\n",
    "    print(f\"Output directory: {resdir}\")\n",
    "    \n",
    "    # Filter prompts to only those that need processing\n",
    "    prompt_list_filtered = [prompt_list[i] for i in indices_to_process]\n",
    "    source_files_filtered = [source_files[i] for i in indices_to_process]\n",
    "    checksums_filtered = [checksums[i] for i in indices_to_process]\n",
    "    buckets_filtered = [buckets[i] for i in indices_to_process]\n",
    "    \n",
    "    if isinstance(system_prompt, list):\n",
    "        system_prompt_filtered = [system_prompt[i] for i in indices_to_process]\n",
    "    else:\n",
    "        system_prompt_filtered = system_prompt\n",
    "    \n",
    "    layers, model_len, is_ft, embed = get_model_info(lma)\n",
    "    total_steering, steering_vec_list = prepare_steering_vectors(\n",
    "        steering_vectors, layer_to_steer, d_model, model_len\n",
    "    )\n",
    "\n",
    "    # Truncate user prompts to 1024 tokens from right\n",
    "    max_tokens = 1024\n",
    "    truncated_prompts = []\n",
    "    for p in prompt_list_filtered:\n",
    "        p_tokens = tokenizer.encode(p, add_special_tokens=False)\n",
    "        if len(p_tokens) > max_tokens:\n",
    "            truncated_tokens = p_tokens[:max_tokens]\n",
    "            truncated_p = tokenizer.decode(truncated_tokens, skip_special_tokens=True)\n",
    "            truncated_prompts.append(truncated_p)\n",
    "        else:\n",
    "            truncated_prompts.append(p)\n",
    "    \n",
    "    # Format prompts with chat template\n",
    "    formatted_string_list = []\n",
    "    if isinstance(system_prompt_filtered, str):\n",
    "        for p in truncated_prompts:\n",
    "            question_string = tokenizer.apply_chat_template(\n",
    "                conversation=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt_filtered},\n",
    "                    {\"role\": \"user\", \"content\": p}\n",
    "                ],\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            formatted_string_list.append(question_string)\n",
    "    else:\n",
    "        assert len(system_prompt_filtered) == len(truncated_prompts)\n",
    "        for p, sys_p in zip(truncated_prompts, system_prompt_filtered):\n",
    "            question_string = tokenizer.apply_chat_template(\n",
    "                conversation=[\n",
    "                    {\"role\": \"system\", \"content\": sys_p},\n",
    "                    {\"role\": \"user\", \"content\": p}\n",
    "                ],\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            formatted_string_list.append(question_string)\n",
    "    \n",
    "    # Create batches\n",
    "    tok_batches = []\n",
    "    prompt_batches = []\n",
    "    system_prompt_batches: List[Union[str, List[str]]] = []\n",
    "    batch_indices_list = []\n",
    "    batch_source_files = []\n",
    "    batch_checksums = []\n",
    "    batch_buckets = []\n",
    "    \n",
    "    for i in range(0, len(formatted_string_list), batch_size):\n",
    "        batch_strings = formatted_string_list[i:i+batch_size]\n",
    "        batch_prompts = prompt_list_filtered[i:i+batch_size]\n",
    "        batch_indices = list(range(i, min(i+batch_size, len(prompt_list_filtered))))\n",
    "        batch_sources = source_files_filtered[i:i+batch_size]\n",
    "        batch_checks = checksums_filtered[i:i+batch_size]\n",
    "        batch_bucket_list = buckets_filtered[i:i+batch_size]\n",
    "        \n",
    "        # Get system prompts for this batch\n",
    "        batch_system_prompts: Union[str, List[str]]\n",
    "        if isinstance(system_prompt_filtered, str):\n",
    "            batch_system_prompts = system_prompt_filtered\n",
    "        else:\n",
    "            batch_system_prompts = system_prompt_filtered[i:i+batch_size]\n",
    "        \n",
    "        tok_batch = tokenizer(\n",
    "            batch_strings, \n",
    "            add_special_tokens=False, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True,\n",
    "            padding_side=\"left\",\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        tok_batches.append(tok_batch)\n",
    "        prompt_batches.append(batch_prompts)\n",
    "        system_prompt_batches.append(batch_system_prompts)\n",
    "        batch_indices_list.append(batch_indices)\n",
    "        batch_source_files.append(batch_sources)\n",
    "        batch_checksums.append(batch_checks)\n",
    "        batch_buckets.append(batch_bucket_list)\n",
    "    \n",
    "    full_responses = []\n",
    "    model_only_responses = []\n",
    "    out_steered_list = []\n",
    "    \n",
    "    batch_iterator = zip(\n",
    "        tok_batches, \n",
    "        prompt_batches, \n",
    "        system_prompt_batches, \n",
    "        batch_indices_list,\n",
    "        batch_source_files, \n",
    "        batch_checksums,\n",
    "        batch_buckets\n",
    "    )\n",
    "    \n",
    "    # Create a single tqdm instance for batches\n",
    "    with tqdm(total=len(tok_batches), desc=\"Processing Batches\") as batch_pbar:\n",
    "        for b_idx, (tok_batch, prompt_batch, sys_prompt_batch, batch_indices, batch_sources, batch_checks, batch_bucket_list) in enumerate(batch_iterator):\n",
    "            tokens_generated = 0\n",
    "            \n",
    "            def update_postfix_callback():\n",
    "                nonlocal tokens_generated\n",
    "                tokens_generated += 1\n",
    "                if tokens_generated % 10 == 0:  # Refresh occasionally to avoid visual lag\n",
    "                    batch_pbar.set_postfix_str(f\"Tokens: {tokens_generated}/{max_new_tokens}\", refresh=False)\n",
    "                    batch_pbar.refresh()\n",
    "            \n",
    "            batch_pbar.set_description(f\"Batch {b_idx+1}/{len(tok_batches)}\")\n",
    "            batch_pbar.set_postfix_str(\"Generating...\", refresh=True)\n",
    "            \n",
    "            # Create user token mask if steering is enabled and user-only steering is requested\n",
    "            user_mask = None\n",
    "            if steering_vectors is not None and steer_on_user:\n",
    "                user_mask = create_user_token_mask(prompt_batch, tok_batch, tokenizer)\n",
    "            \n",
    "            # Create system token mask if steering is enabled and system-only steering is requested\n",
    "            system_mask = None\n",
    "            if steering_vectors is not None and steer_on_system:\n",
    "                system_mask = create_system_token_mask(sys_prompt_batch, tok_batch, tokenizer)\n",
    "            \n",
    "            # Generate with or without steering\n",
    "            if steering_vectors is None:\n",
    "                with lma.generate(tok_batch, max_new_tokens=max_new_tokens, temperature=temperature, pad_token_id=tokenizer.pad_token_id, top_p=top_p) as gen:\n",
    "                    out_steered = lma.generator.output.save()\n",
    "            elif (steer_on_user or steer_on_system) and not steer_on_thinking:\n",
    "                steering_vec_list = cast(List[torch.Tensor], steering_vec_list)\n",
    "                \n",
    "                # Create combined mask for user and/or system tokens\n",
    "                combined_mask = None\n",
    "                if steer_on_user and steer_on_system:\n",
    "                    user_mask = cast(torch.Tensor, user_mask)\n",
    "                    system_mask = cast(torch.Tensor, system_mask)\n",
    "                    combined_mask = torch.logical_or(user_mask, system_mask)\n",
    "                elif steer_on_user:\n",
    "                    combined_mask = cast(torch.Tensor, user_mask)\n",
    "                elif steer_on_system:\n",
    "                    combined_mask = cast(torch.Tensor, system_mask)\n",
    "                \n",
    "                # Simple case: only steer on user/system tokens, no thinking steering\n",
    "                with lma.generate(tok_batch, max_new_tokens=max_new_tokens, temperature=temperature, pad_token_id=tokenizer.pad_token_id, top_p=top_p) as gen:\n",
    "                    # Apply steering to user/system tokens only at the beginning\n",
    "                    if layer_to_steer == 'all':\n",
    "                        for i in range(model_len):\n",
    "                            apply_steering_to_layer(layers[i], steering_vec_list[i], combined_mask)\n",
    "                    elif isinstance(layer_to_steer, list):\n",
    "                        for i in layer_to_steer:\n",
    "                            apply_steering_to_layer(layers[i], steering_vec_list[i], combined_mask)\n",
    "                    else:  # Single layer\n",
    "                        apply_steering_to_layer(layers[layer_to_steer], total_steering, combined_mask)\n",
    "                    \n",
    "                    out_steered = lma.generator.output.save()\n",
    "            else:\n",
    "                steering_vec_list = cast(List[torch.Tensor], steering_vec_list)\n",
    "                \n",
    "                # Create initial combined mask for user and/or system tokens\n",
    "                initial_mask = None\n",
    "                if steer_on_user and steer_on_system:\n",
    "                    user_mask = cast(torch.Tensor, user_mask)\n",
    "                    system_mask = cast(torch.Tensor, system_mask)\n",
    "                    initial_mask = torch.logical_or(user_mask, system_mask)\n",
    "                elif steer_on_user:\n",
    "                    initial_mask = cast(torch.Tensor, user_mask)\n",
    "                elif steer_on_system:\n",
    "                    initial_mask = cast(torch.Tensor, system_mask)\n",
    "                \n",
    "                # Complex case with thinking steering\n",
    "                mask_for_first_period = torch.zeros(tok_batch['input_ids'].shape[0], dtype=torch.bool, device=\"cuda\")\n",
    "                max_phase1_tokens = min(150, max_new_tokens)\n",
    "                \n",
    "                with lma.generate(tok_batch, max_new_tokens=max_phase1_tokens, temperature=temperature, pad_token_id=tokenizer.pad_token_id, top_p=top_p) as gen:\n",
    "                    for j in range(max_phase1_tokens):\n",
    "                        batch_pbar.set_postfix_str(f\"Tokens: {j + 1}/{max_new_tokens}\", refresh=True)\n",
    "                        \n",
    "                        if j == 0:\n",
    "                            if steer_on_user or steer_on_system:\n",
    "                                if layer_to_steer == 'all':\n",
    "                                    for i in range(model_len):\n",
    "                                        apply_steering_to_layer(layers[i], steering_vec_list[i], initial_mask)\n",
    "                                        layers[i].next()\n",
    "                                elif isinstance(layer_to_steer, list):\n",
    "                                    for i in layer_to_steer:\n",
    "                                        apply_steering_to_layer(layers[i], steering_vec_list[i], initial_mask)\n",
    "                                        layers[i].next()\n",
    "                                else:\n",
    "                                    apply_steering_to_layer(layers[layer_to_steer], total_steering, initial_mask)\n",
    "                                    layers[layer_to_steer].next()\n",
    "                            else:\n",
    "                                if layer_to_steer == 'all':\n",
    "                                    for i in range(model_len):\n",
    "                                        layers[i].next()\n",
    "                                elif isinstance(layer_to_steer, list):\n",
    "                                    for i in layer_to_steer:\n",
    "                                        layers[i].next()\n",
    "                                else:\n",
    "                                    layers[layer_to_steer].next()\n",
    "                        else:\n",
    "                            if steer_on_thinking:\n",
    "                                cur_period = embed.input.squeeze() == 13\n",
    "                                mask_for_first_period = torch.logical_or(cur_period, mask_for_first_period.detach())\n",
    "                                if layer_to_steer == 'all':\n",
    "                                    for i in range(model_len):\n",
    "                                        apply_steering_to_layer(layers[i], steering_vec_list[i], mask_for_first_period.unsqueeze(-1))\n",
    "                                        layers[i].next()\n",
    "                                elif isinstance(layer_to_steer, list):\n",
    "                                    for i in layer_to_steer:\n",
    "                                        apply_steering_to_layer(layers[i], steering_vec_list[i], mask_for_first_period.unsqueeze(-1))\n",
    "                                        layers[i].next()\n",
    "                                else:\n",
    "                                    apply_steering_to_layer(layers[layer_to_steer], total_steering, mask_for_first_period.unsqueeze(-1))\n",
    "                                    layers[layer_to_steer].next()\n",
    "                        embed.next()\n",
    "                    \n",
    "                    phase1_output = lma.generator.output.save()\n",
    "                \n",
    "                remaining_tokens = max_new_tokens - max_phase1_tokens\n",
    "                \n",
    "                if remaining_tokens > 0:\n",
    "                    # Find where we've seen periods in phase 1\n",
    "                    batch_size = phase1_output.shape[0]\n",
    "                    period_token_id = 13\n",
    "                    \n",
    "                    # Create mask for phase 2 - need to track which positions had periods\n",
    "                    phase2_length = phase1_output.shape[1]\n",
    "                    phase2_mask_for_first_period = torch.zeros((batch_size, phase2_length), dtype=torch.bool, device=\"cuda\")\n",
    "                    \n",
    "                    # Find positions after first period for each sequence\n",
    "                    for b in range(batch_size):\n",
    "                        period_positions = (phase1_output[b] == period_token_id).nonzero(as_tuple=True)[0]\n",
    "                        if len(period_positions) > 0:\n",
    "                            first_period_pos = period_positions[0].item()\n",
    "                            phase2_mask_for_first_period[b, first_period_pos + 1:] = True\n",
    "                    \n",
    "                    # Create attention mask for phase 2\n",
    "                    phase2_attention_mask = torch.ones_like(phase1_output, dtype=torch.long, device=\"cuda\")\n",
    "                    if 'attention_mask' in tok_batch:\n",
    "                        orig_mask_length = tok_batch['attention_mask'].shape[1]\n",
    "                        phase2_attention_mask[:, :orig_mask_length] = tok_batch['attention_mask']\n",
    "                    \n",
    "                    # Continue generation with phase 2\n",
    "                    phase2_input = {\n",
    "                        'input_ids': phase1_output,\n",
    "                        'attention_mask': phase2_attention_mask\n",
    "                    }\n",
    "                    \n",
    "                    with lma.generate(phase2_input, max_new_tokens=remaining_tokens, temperature=temperature, pad_token_id=tokenizer.pad_token_id, top_p=top_p) as gen:\n",
    "                        # Continue with the same pattern using .next()\n",
    "                        for i in range(remaining_tokens):\n",
    "                            total_tokens = max_phase1_tokens + i + 1\n",
    "                            batch_pbar.set_postfix_str(f\"Tokens: {total_tokens}/{max_new_tokens}\", refresh=True)\n",
    "                            \n",
    "                            if i == 0:\n",
    "                                # Apply initial steering to the existing sequence based on our masks\n",
    "                                if (steer_on_user or steer_on_system) and initial_mask is not None:\n",
    "                                    # Create combined mask for initial steering\n",
    "                                    combined_initial_mask = torch.zeros((batch_size, phase2_length), dtype=torch.bool, device=\"cuda\")\n",
    "                                    initial_mask_length = initial_mask.shape[1]\n",
    "                                    combined_initial_mask[:, :initial_mask_length] = initial_mask\n",
    "                                    combined_initial_mask = torch.logical_or(combined_initial_mask, phase2_mask_for_first_period)\n",
    "                                    \n",
    "                                    if layer_to_steer == 'all':\n",
    "                                        for l in range(model_len):\n",
    "                                            apply_steering_to_layer(layers[l], steering_vec_list[l], combined_initial_mask)\n",
    "                                            layers[l].next()\n",
    "                                    elif isinstance(layer_to_steer, list):\n",
    "                                        for l in layer_to_steer:\n",
    "                                            apply_steering_to_layer(layers[l], steering_vec_list[l], combined_initial_mask)\n",
    "                                            layers[l].next()\n",
    "                                    else:\n",
    "                                        apply_steering_to_layer(layers[layer_to_steer], total_steering, combined_initial_mask)\n",
    "                                        layers[layer_to_steer].next()\n",
    "                                else:\n",
    "                                    # Just apply thinking steering\n",
    "                                    if layer_to_steer == 'all':\n",
    "                                        for l in range(model_len):\n",
    "                                            apply_steering_to_layer(layers[l], steering_vec_list[l], phase2_mask_for_first_period)\n",
    "                                            layers[l].next()\n",
    "                                    elif isinstance(layer_to_steer, list):\n",
    "                                        for l in layer_to_steer:\n",
    "                                            apply_steering_to_layer(layers[l], steering_vec_list[l], phase2_mask_for_first_period)\n",
    "                                            layers[l].next()\n",
    "                                    else:\n",
    "                                        apply_steering_to_layer(layers[layer_to_steer], total_steering, phase2_mask_for_first_period)\n",
    "                                        layers[layer_to_steer].next()\n",
    "                            else:\n",
    "                                # For subsequent tokens, apply thinking steering to all\n",
    "                                if steer_on_thinking:\n",
    "                                    if layer_to_steer == 'all':\n",
    "                                        for l in range(model_len):\n",
    "                                            layers[l].output[:,:,:] = layers[l].output[:,:,:] + steering_vec_list[l].unsqueeze(0).unsqueeze(0)\n",
    "                                            layers[l].next()\n",
    "                                    elif isinstance(layer_to_steer, list):\n",
    "                                        for l in layer_to_steer:\n",
    "                                            layers[l].output[:,:,:] = layers[l].output[:,:,:] + steering_vec_list[l].unsqueeze(0).unsqueeze(0)\n",
    "                                            layers[l].next()\n",
    "                                    else:\n",
    "                                        layers[layer_to_steer].output[:,:,:] = layers[layer_to_steer].output[:,:,:] + total_steering.unsqueeze(0).unsqueeze(0)\n",
    "                                        layers[layer_to_steer].next()\n",
    "                            embed.next()\n",
    "                        \n",
    "                        out_steered = lma.generator.output.save()\n",
    "                else:\n",
    "                    out_steered = phase1_output\n",
    "            \n",
    "            batch_pbar.set_postfix_str(\"Decoding & Saving...\", refresh=True)\n",
    "            \n",
    "            out_steered_list.append(out_steered)\n",
    "            \n",
    "            # Decode responses\n",
    "            full_decoded = tokenizer.batch_decode(out_steered)\n",
    "            full_decoded = [d.replace(tokenizer.eos_token, '').replace('<|end_of_text|>', '') for d in full_decoded]\n",
    "            full_responses.extend(full_decoded)\n",
    "            \n",
    "            # Decode model-only responses\n",
    "            model_only_decoded = []\n",
    "            for i, full_output in enumerate(out_steered):\n",
    "                input_length = tok_batch['input_ids'][i].shape[0]\n",
    "                model_only_output = tokenizer.decode(full_output[input_length:])\n",
    "                model_only_output = model_only_output.replace(tokenizer.eos_token, '').replace('<|end_of_text|>', '')\n",
    "                model_only_decoded.append(model_only_output)\n",
    "            \n",
    "            model_only_responses.extend(model_only_decoded)\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # Save results with source file-based naming\n",
    "            if (b_idx + 1) % backup_every == 0:\n",
    "                for i, local_idx in enumerate(batch_indices):\n",
    "                    source_file = batch_sources[i]\n",
    "                    checksum = batch_checks[i]\n",
    "                    \n",
    "                    # Create output filename based on source file with seed\n",
    "                    base_name = os.path.splitext(os.path.basename(source_file))[0]\n",
    "                    out_filename = f\"{base_name}_seed_{seed}_steer_out.yaml\"\n",
    "                    out_path = os.path.join(resdir, out_filename)\n",
    "                    \n",
    "                    # Gather system prompt for this sample\n",
    "                    sys_p = sys_prompt_batch if isinstance(sys_prompt_batch, str) else sys_prompt_batch[i]\n",
    "                    \n",
    "                    data = {\n",
    "                        \"source_file\": source_file,\n",
    "                        \"checksum\": checksum,\n",
    "                        \"seed\": seed,\n",
    "                        \"prompt\": prompt_batch[i],\n",
    "                        \"system_prompt\": sys_p,\n",
    "                        \"full_response\": full_decoded[i],\n",
    "                        \"model_only_response\": model_only_decoded[i],\n",
    "                    }\n",
    "                    \n",
    "                    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                        yaml.safe_dump(data, f, allow_unicode=True, sort_keys=False)\n",
    "            \n",
    "            batch_pbar.update(1)\n",
    "    \n",
    "    return full_responses, model_only_responses, tok_batches, out_steered_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
